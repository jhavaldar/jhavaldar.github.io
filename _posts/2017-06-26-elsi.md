---
layout: post
title: "Elements of Statistical Learning"
date:   2017-06-26
mathjax: true
tags: [statistics]
category: [data science]
---

Here begins my notes for _The Elements of Statistical Learning_ by Hastie, Tibshirani, and Friedman.

## 2 Overview of Supervised Learning

### 2.2 Variable types

**Regression** predicts discrete or continuous input (ex: scalars), while **classification** predicts categorical input (ex: yes or no)

### 2.3 Least Squares and Nearest Neighbors

#### Linear models and least squares

Given a vector of inputs $X^{T}=(X_1, X_2,...,X_p)$, we predict the ouput $Y$ with the following function:

$$ Y = \beta_0 + \sum_{j=1}^{p}X_J\beta_j$$

Where $\beta_0$ is temrmed the intercept of bias. We can more succinctly pad $X$ with an extra $1$, and then this is simply the dot product of two vectors:

$$ Y = X^{T}\beta $$

This generates a scalar output; more generally, to obtain a $k$-vector output, we use matrix multiplication, letting $\beta$ be a $p \times K$ matrix of coefficients.

How does one pick $\beta$? One possible cost is the residual sum of squares (RSS), the square norm of the vector $Y - X \beta$

$$ \text{RSS}(\beta) = (y-X \beta)\cdot(y-X \beta)$$

RSS is a quadratic function, and so it has a minimum, though not necessarily a unique one. To find this minimum, we simply differentiate with respect to $\beta$, remembering our differentiation rules for vectors:

$$ X\cdot(y-X\beta) = 0 $$
$$ X^Ty = X^{T}X\beta $$
$$ (X^{T}X)^{-1}X^{T}y = \beta $$

This is assuming that $X^{T}X$ is nonsingular. Note that $X$ is singular if and only if its determinant is $0$, so in fact we only require $X$ nonsingular.
---